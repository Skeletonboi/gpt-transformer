# gpt2-plus
PyTorch implementation and reproduction of OpenAI GPT-2 architecture (124M scale) with training code from scratch. Future work include instruction finetuning of the model through direct Supervised Finetuning (SFT) or by implementation of Parameter Efficient Finetuning (PEFT) methods such as (Q)Lora and/or IA3 and RAG from scratch.
