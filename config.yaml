---
# Training config for GPT-2 architecture
n_context : 1024 # context length
n_vocab : 50257 # num tokens in vocabulary

n_layer : 12 # num transformer blocks
n_head : 12 # num heads in multi-head attention
n_embed : 768 # embedding dimension within attention block

use_flash_attn : True
use_dropout : False
batch_size : 4
n_epoch : 165 # LIMA ~680k tokens, / (4*1024) = ~165 epochs to see all data once
lr : 3e-4