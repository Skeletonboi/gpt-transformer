---
# Training config for GPT-2 architecture
block_size : 1024 # context length
vocab_size : 50257 # num tokens in vocabulary
embed_size : 768 # embedding dimension

n_layer : 12 # num transformer blocks
n_head : 12 # num heads in multi-head attention
n_embd : 768 # embedding dimension within attention block